# @autobe/estimate

A comprehensive code quality evaluation tool for AutoBE-generated code.

## Table of Contents

- [Overview](#overview)
- [What is AutoBE?](#what-is-autobe)
- [Why This Tool?](#why-this-tool)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [CLI Reference](#cli-reference)
- [Evaluation System](#evaluation-system)
- [Detailed Evaluation Criteria](#detailed-evaluation-criteria)
- [Scoring System](#scoring-system)
- [Target Folders](#target-folders)
- [Output Reports](#output-reports)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)
- [Project Structure](#project-structure)
- [License](#license)

---

## Overview

`@autobe/estimate` is a static analysis tool designed specifically for evaluating code generated by AutoBE. It analyzes TypeScript/NestJS projects across multiple dimensions including syntax correctness, code quality, security, and LLM-specific issues.

---

## What is AutoBE?

AutoBE is an automated backend code generation system that uses Large Language Models (LLMs) to generate complete backend applications. It follows a 5-phase generation process:

| Phase | Name | Description | Output |
|-------|------|-------------|--------|
| 1 | Analyze | Analyzes requirements | `docs/analysis/` |
| 2 | Database | Designs database schema | `prisma/schema/` |
| 3 | Interface | Creates API interfaces | `src/controllers/`, `src/api/structures/` |
| 4 | Test | Generates E2E tests | `test/features/api/` |
| 5 | Realize | Implements business logic | `src/providers/` |

---

## Why This Tool?

LLM-generated code often contains issues that differ from human-written code:

- **Hallucinated imports**: Importing packages that don't exist
- **Incomplete implementations**: `throw new Error("not implemented")`
- **TODO comments left behind**: Placeholders never filled in
- **Security issues**: Hardcoded credentials, eval() usage
- **High complexity**: Overly complex functions

This tool detects these issues and provides a quality score.

---

## Installation

### Prerequisites

- Node.js >= 18.0.0
- pnpm >= 8.0.0

### Steps
```bash
# Clone the repository
git clone <repository-url>
cd autobe-project

# Install dependencies
pnpm install

# Navigate to estimate package
cd packages/estimate
```

---

## Quick Start
```bash
# Evaluate a project
pnpm start --input /path/to/autobe-project --output ./reports --verbose

# Example with autobe-examples
pnpm start --input ~/autobe-examples/anthropic/claude-sonnet-4.5/todo --output ./reports --verbose
```

---

## CLI Reference

### Basic Syntax
```bash
pnpm start --input <path> --output <path> [options]
```

### Options

| Option | Alias | Description | Required | Default |
|--------|-------|-------------|----------|---------|
| `--input` | `-i` | Path to the project to evaluate | Yes | - |
| `--output` | `-o` | Path to save report files | Yes | - |
| `--verbose` | `-v` | Enable detailed logging | No | false |
| `--continue-on-gate-failure` | - | Continue evaluation even if Gate phase fails | No | false |

### Examples
```bash
# Basic evaluation
pnpm start -i ./my-project -o ./reports

# Verbose mode
pnpm start -i ./my-project -o ./reports -v

# Continue on Gate failure (useful for debugging)
pnpm start -i ./my-project -o ./reports --continue-on-gate-failure
```

---

## Evaluation System

### Phase Overview

The evaluation follows AutoBE's 5-phase structure:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GATE                                 â”‚
â”‚              (Must Pass - Otherwise Score = 0)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Syntax    â”‚ â”‚    Type     â”‚ â”‚   Prisma    â”‚            â”‚
â”‚  â”‚  Evaluator  â”‚ â”‚  Evaluator  â”‚ â”‚  Evaluator  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCORING PHASES                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Functionality  â”‚  â”‚     Quality     â”‚                   â”‚
â”‚  â”‚      40%        â”‚  â”‚       30%       â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚     Safety      â”‚  â”‚   LLM Specific  â”‚                   â”‚
â”‚  â”‚      20%        â”‚  â”‚       10%       â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Total Score  â”‚
                    â”‚   (0-100)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Grading Scale

| Grade | Score Range | Description |
|-------|-------------|-------------|
| A | 90 - 100 | Excellent - Production ready |
| B | 80 - 89 | Good - Minor improvements needed |
| C | 70 - 79 | Average - Several issues to address |
| D | 60 - 69 | Below Average - Significant issues |
| F | 0 - 59 | Fail - Major issues or Gate failure |

---

## Detailed Evaluation Criteria

### 1. Gate Phase (Pass/Fail)

**Purpose**: Ensure basic code validity before detailed analysis.

> âš ï¸ If Gate fails, total score = 0

| Evaluator | What it Checks | Failure Condition |
|-----------|----------------|-------------------|
| **SyntaxEvaluator** | TypeScript syntax validity | Any file fails to parse |
| **TypeEvaluator** | TypeScript type correctness | `tsc --noEmit` returns errors |
| **PrismaEvaluator** | Prisma schema validity | `prisma validate` fails |

---

### 2. Functionality Phase (40% Weight)

**Purpose**: Evaluate requirements coverage and test existence.

**AutoBE Phase**: Analyze, Test

| Evaluator | What it Checks | Scoring |
|-----------|----------------|---------|
| **RequirementsEvaluator** | Existence of `docs/analysis/` | Exists: 100, Missing: 80 |
| **TestRunnerEvaluator** | Existence of test files in `test/` | Exists: 80, Missing: 50 |

**Final Score**: Average of both evaluators

---

### 3. Quality Phase (30% Weight)

**Purpose**: Evaluate code quality and maintainability.

**AutoBE Phase**: Database, Interface, Realize

| Evaluator | What it Checks | Issues |
|-----------|----------------|--------|
| **ComplexityEvaluator** | Cyclomatic complexity of functions | C001 (Critical): >20, C002 (Warning): >15 |
| **NamingEvaluator** | Naming conventions | N001: Class not PascalCase, N002: Interface not PascalCase |
| **JsDocEvaluator** | JSDoc documentation on classes/interfaces | J001: Missing class JSDoc, J002: Missing interface JSDoc |
| **DuplicationEvaluator** | Duplicate code blocks (â‰¥6 lines) | D001: Duplicate code detected |

#### Complexity Thresholds
```
Complexity 1-15:  âœ… OK
Complexity 16-20: âš ï¸ Warning (C002)
Complexity 21+:   âŒ Critical (C001)
```

---

### 4. Safety Phase (20% Weight)

**Purpose**: Detect security vulnerabilities and error handling issues.

**AutoBE Phase**: All phases

| Evaluator | What it Checks | Issues |
|-----------|----------------|--------|
| **SecurityEvaluator** | Security vulnerabilities | S001-S005 |
| **ErrorHandlingEvaluator** | Error handling patterns | E001-E002 |
| **ValidationEvaluator** | Input validation | V001 |

#### Security Patterns Detected

| Code | Severity | Pattern | Risk |
|------|----------|---------|------|
| S001 | Critical | `password = "..."` | Hardcoded password |
| S002 | Critical | `api_key = "..."` | Hardcoded API key |
| S003 | Critical | `secret = "..."` | Hardcoded secret |
| S004 | Critical | `eval(...)` | Code injection risk |
| S005 | Warning | `innerHTML = ...` | XSS vulnerability |

#### Error Handling Patterns

| Code | Severity | Pattern | Issue |
|------|----------|---------|-------|
| E001 | Warning | `catch { }` | Empty catch block |
| E002 | Warning | `.then()` without `.catch()` | Unhandled Promise rejection |

---

### 5. LLM Specific Phase (10% Weight)

**Purpose**: Detect issues commonly found in LLM-generated code.

**AutoBE Phase**: All phases

| Evaluator | What it Checks | Issues |
|-----------|----------------|--------|
| **HallucinationEvaluator** | Non-existent package imports | H001: Package not in package.json |
| **TodoEvaluator** | TODO/FIXME comments | T001 (Warning): TODO, T002 (Critical): FIXME |
| **IncompleteEvaluator** | Incomplete implementations | I001-I002 |

#### Hallucination Detection

Checks if imported packages exist in `package.json`:
```typescript
// âŒ H001: Package "@fake/package" not found in package.json
import { something } from "@fake/package";
```

**Allowed packages** (not flagged even if not in package.json):
- `@prisma/client`, `@prisma/sdk`
- `@nestia/core`, `@nestia/fetcher`
- `@nestjs/common`, `@nestjs/core`
- Node.js built-ins (`fs`, `path`, `http`, etc.)

#### Incomplete Implementation Patterns

| Code | Severity | Pattern |
|------|----------|---------|
| I001 | Critical | `throw new Error("not implemented")` |
| I002 | Critical | `// implement this` comment |

---

## Scoring System

### Severity Weights

Each issue has a severity that affects the score:

| Severity | Weight | Description |
|----------|--------|-------------|
| Critical | -10 | Must fix - blocks production use |
| Warning | -3 | Should fix - affects quality |
| Suggestion | -1 | Nice to fix - minor improvement |

### Phase Score Calculation
```
weightedIssues = (critical Ã— 10) + (warning Ã— 3) + (suggestion Ã— 1)
issueRatio = weightedIssues / totalFiles
phaseScore = max(0, min(100, 100 - issueRatio Ã— 10))
```

### Total Score Calculation
```
totalScore = (functionality Ã— 0.4) + (quality Ã— 0.3) + (safety Ã— 0.2) + (llmSpecific Ã— 0.1)
```

> If Gate fails, totalScore = 0

### Example Calculation
```
Files: 50
Critical Issues: 2
Warning Issues: 10
Suggestions: 5

weightedIssues = (2 Ã— 10) + (10 Ã— 3) + (5 Ã— 1) = 20 + 30 + 5 = 55
issueRatio = 55 / 50 = 1.1
phaseScore = 100 - (1.1 Ã— 10) = 89
```

---

## Target Folders

The tool scans specific folders based on AutoBE's output structure:

| Folder | AutoBE Phase | Contents | Evaluations |
|--------|--------------|----------|-------------|
| `src/controllers/` | Interface | API endpoint handlers | Quality, Safety, LLM |
| `src/providers/` | Realize | Business logic | Quality, Safety, LLM |
| `src/api/structures/` | Interface | DTOs, request/response types | Quality, LLM |
| `test/features/api/` | Test | E2E test files | Functionality (count only) |
| `prisma/schema/` | Database | Prisma schema files | Gate (Prisma validation) |
| `docs/analysis/` | Analyze | Requirements documents | Functionality |

### Ignored Paths

- `**/node_modules/**`
- `**/dist/**`
- `**/*.d.ts`
- `**/*.js` (only `.ts` files are evaluated)

---

## Output Reports

Two report files are generated:

### 1. Markdown Report (`estimate-report.md`)

Human-readable report with:
- Overall score and grade
- Phase-by-phase breakdown
- List of all issues with locations
- Summary statistics

### 2. JSON Report (`estimate-report.json`)

Machine-readable report for CI/CD integration:
```json
{
  "targetPath": "/path/to/project",
  "totalScore": 74,
  "grade": "C",
  "phases": {
    "gate": { "passed": true, "score": 100 },
    "functionality": { "score": 90 },
    "quality": { "score": 25 },
    "safety": { "score": 100 },
    "llmSpecific": { "score": 100 }
  },
  "summary": {
    "totalIssues": 156,
    "criticalCount": 0,
    "warningCount": 147,
    "suggestionCount": 9
  }
}
```

---

## Examples

### Example 1: Evaluating Claude-generated Code
```bash
pnpm start \
  --input ~/autobe-examples/anthropic/claude-sonnet-4.5/todo \
  --output ./reports \
  --verbose
```

**Output:**
```
ğŸ“Š Total Score: 74/100 (Grade: C)

   Gate:          âœ… Pass
   Functionality: 90/100
   Quality:       25/100
   Safety:        100/100
   LLM Specific:  100/100

âš ï¸  Warnings: 947
ğŸ’¡ Suggestions: 9
```

### Example 2: Comparing Multiple Models
```bash
# Claude Sonnet 4.5
pnpm start -i ~/autobe-examples/anthropic/claude-sonnet-4.5/todo -o ./reports/claude

# GPT-4.1
pnpm start -i ~/autobe-examples/openai/gpt-4.1/todo -o ./reports/gpt

# Gemini 2.5 Pro
pnpm start -i ~/autobe-examples/google/gemini-2.5-pro/todo -o ./reports/gemini
```

### Example 3: CI/CD Integration
```yaml
# .github/workflows/evaluate.yml
- name: Evaluate Generated Code
  run: |
    pnpm start -i ./generated -o ./reports
    SCORE=$(cat ./reports/estimate-report.json | jq '.totalScore')
    if [ $SCORE -lt 70 ]; then
      echo "Quality score too low: $SCORE"
      exit 1
    fi
```

---

## Troubleshooting

### Gate Fails: Prisma Validation Error

**Problem**: `[P001] Prisma schema validation failed`

**Solution**:
1. Check if `prisma/schema/` exists
2. Run `npx prisma validate` manually to see detailed error
3. Ensure Prisma CLI is installed

### Gate Fails: Type Errors

**Problem**: `tsc --noEmit` returns errors

**Solution**:
1. Check if `tsconfig.json` exists
2. Run `npx tsc --noEmit` manually to see errors
3. Ensure all dependencies are installed (`pnpm install`)

### No Files Found

**Problem**: `Found 0 TypeScript files`

**Solution**:
1. Verify the input path is correct
2. Check if the project follows AutoBE folder structure
3. Ensure files have `.ts` extension

### Use --continue-on-gate-failure

To debug issues, run with this flag to see all evaluation results:
```bash
pnpm start -i ./project -o ./reports --continue-on-gate-failure -v
```

---

## Project Structure
```
packages/estimate/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bin/
â”‚   â”‚   â””â”€â”€ estimate.ts          # CLI entry point
â”‚   â”œâ”€â”€ cli.ts                   # CLI argument parsing
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ context-builder.ts   # Project scanning
â”‚   â”‚   â””â”€â”€ pipeline.ts          # Evaluation orchestration
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ base.ts              # Base evaluator classes
â”‚   â”‚   â”œâ”€â”€ gate/                # Gate evaluators
â”‚   â”‚   â”œâ”€â”€ quality/             # Quality evaluators
â”‚   â”‚   â”œâ”€â”€ safety/              # Safety evaluators
â”‚   â”‚   â”œâ”€â”€ llm-specific/        # LLM-specific evaluators
â”‚   â”‚   â””â”€â”€ functionality/       # Functionality evaluators
â”‚   â”œâ”€â”€ reporters/
â”‚   â”‚   â”œâ”€â”€ json.reporter.ts     # JSON report generator
â”‚   â”‚   â””â”€â”€ markdown.reporter.ts # Markdown report generator
â”‚   â”œâ”€â”€ types/                   # TypeScript type definitions
â”‚   â””â”€â”€ index.ts                 # Package exports
â”œâ”€â”€ reports/                     # Generated reports
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ evaluation-criteria.json     # Evaluation criteria config
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

---

## License

MIT