# 코드 평가 에이전트

A code quality evaluation tool for AutoBE-generated code.

## Table of Contents

- [Overview](#overview)
- [What is AutoBE?](#what-is-autobe)
- [Why This Tool?](#why-this-tool)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [CLI Reference](#cli-reference)
- [Evaluation System](#evaluation-system)
- [Detailed Evaluation Criteria](#detailed-evaluation-criteria)
- [Scoring System](#scoring-system)
- [Target Folders](#target-folders)
- [Output Reports](#output-reports)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)
- [Project Structure](#project-structure)
- [License](#license)

---

## Overview

`@autobe/estimate` is a static analysis tool designed specifically for evaluating code generated by AutoBE. It analyzes TypeScript/NestJS projects across multiple dimensions including syntax correctness, code quality, security, and LLM-specific issues.

---

## What is AutoBE?

AutoBE is an automated backend code generation system that uses Large Language Models (LLMs) to generate complete backend applications. It follows a 5-phase generation process:

| Phase | Name | Description | Output |
|-------|------|-------------|--------|
| 1 | Analyze | Analyzes requirements | `docs/analysis/` |
| 2 | Database | Designs database schema | `prisma/schema/` |
| 3 | Interface | Creates API interfaces | `src/controllers/`, `src/api/structures/` |
| 4 | Test | Generates E2E tests | `test/features/api/` |
| 5 | Realize | Implements business logic | `src/providers/` |

---

## Why This Tool?

LLM-generated code often contains issues that differ from human-written code:

- Hallucinated imports: Importing packages that don't exist
- Incomplete implementations: `throw new Error("not implemented")`
- TODO comments left behind: Placeholders never filled in
- Security issues: Hardcoded credentials, eval() usage
- High complexity: Overly complex functions

This tool detects these issues and provides a quality score.

---

## Installation

You'll need Node.js 18+ and pnpm 8+ installed.

```bash
# Clone the repository
git clone <repository-url>
cd autobe-project

# Install dependencies
pnpm install

# Navigate to estimate package
cd packages/estimate
```

---

## Quick Start

```bash
# Evaluate a project
pnpm start --input /path/to/autobe-project --output ./reports --verbose

# Example with autobe-examples
pnpm start --input ~/autobe-examples/anthropic/claude-sonnet-4.5/todo --output ./reports --verbose
```

---

## CLI Reference

Basic syntax:

```bash
pnpm start --input <path> --output <path> [options]
```

Options:

| Option | Alias | Description | Required | Default |
|--------|-------|-------------|----------|---------|
| `--input` | `-i` | Path to the project to evaluate | Yes | - |
| `--output` | `-o` | Path to save report files | Yes | - |
| `--verbose` | `-v` | Enable detailed logging | No | false |
| `--continue-on-gate-failure` | - | Continue evaluation even if Gate phase fails | No | false |

Some examples:

```bash
# Basic evaluation
pnpm start -i ./my-project -o ./reports

# Verbose mode
pnpm start -i ./my-project -o ./reports -v

# Continue on Gate failure (useful for debugging)
pnpm start -i ./my-project -o ./reports --continue-on-gate-failure
```

---

## Evaluation System

### Phase Overview

The evaluation follows AutoBE's 5-phase structure. First, there's a Gate phase that must pass - if it fails, the total score is 0. The Gate phase runs three evaluators: Syntax Evaluator, Type Evaluator, and Prisma Evaluator.

Once the Gate passes, the scoring phases kick in. Functionality accounts for 40% of the score, Quality for 30%, Safety for 20%, and LLM Specific for 10%. These all feed into the final total score, which ranges from 0 to 100.

### Grading Scale

| Grade | Score Range | Description |
|-------|-------------|-------------|
| A | 90 - 100 | Excellent - Production ready |
| B | 80 - 89 | Good - Minor improvements needed |
| C | 70 - 79 | Average - Several issues to address |
| D | 60 - 69 | Below Average - Significant issues |
| F | 0 - 59 | Fail - Major issues or Gate failure |

---

## Detailed Evaluation Criteria

### 1. Gate Phase (Pass/Fail)

The Gate phase ensures basic code validity before detailed analysis. If Gate fails, total score = 0.

| Evaluator | What it Checks | Failure Condition |
|-----------|----------------|-------------------|
| SyntaxEvaluator | TypeScript syntax validity | Any file fails to parse |
| TypeEvaluator | TypeScript type correctness | `tsc --noEmit` returns errors |
| PrismaEvaluator | Prisma schema validity | `prisma validate` fails |

---

### 2. Functionality Phase (40% Weight)

This phase evaluates requirements coverage and test existence. It corresponds to AutoBE's Analyze and Test phases.

| Evaluator | What it Checks | Scoring |
|-----------|----------------|---------|
| RequirementsEvaluator | Existence of `docs/analysis/` | Exists: 100, Missing: 80 |
| TestRunnerEvaluator | Existence of test files in `test/` | Exists: 80, Missing: 50 |

Final Score is the average of both evaluators.

---

### 3. Quality Phase (30% Weight)

This phase evaluates code quality and maintainability. It corresponds to AutoBE's Database, Interface, and Realize phases.

| Evaluator | What it Checks | Issues |
|-----------|----------------|--------|
| ComplexityEvaluator | Cyclomatic complexity of functions | C001 (Critical): >20, C002 (Warning): >15 |
| NamingEvaluator | Naming conventions | N001: Class not PascalCase, N002: Interface not PascalCase |
| JsDocEvaluator | JSDoc documentation on classes/interfaces | J001: Missing class JSDoc, J002: Missing interface JSDoc |
| DuplicationEvaluator | Duplicate code blocks (6+ lines) | D001: Duplicate code detected |

Complexity thresholds work like this: complexity between 1-15 is fine, 16-20 triggers a warning (C002), and 21+ is critical (C001).

---

### 4. Safety Phase (20% Weight)

This phase detects security vulnerabilities and error handling issues. It applies to all AutoBE phases.

| Evaluator | What it Checks | Issues |
|-----------|----------------|--------|
| SecurityEvaluator | Security vulnerabilities | S001-S005 |
| ErrorHandlingEvaluator | Error handling patterns | E001-E002 |
| ValidationEvaluator | Input validation | V001 |

Security patterns detected:

| Code | Severity | Pattern | Risk |
|------|----------|---------|------|
| S001 | Critical | `password = "..."` | Hardcoded password |
| S002 | Critical | `api_key = "..."` | Hardcoded API key |
| S003 | Critical | `secret = "..."` | Hardcoded secret |
| S004 | Critical | `eval(...)` | Code injection risk |
| S005 | Warning | `innerHTML = ...` | XSS vulnerability |

Error handling patterns:

| Code | Severity | Pattern | Issue |
|------|----------|---------|-------|
| E001 | Warning | `catch { }` | Empty catch block |
| E002 | Warning | `.then()` without `.catch()` | Unhandled Promise rejection |

---

### 5. LLM Specific Phase (10% Weight)

This phase detects issues commonly found in LLM-generated code. It applies to all AutoBE phases.

| Evaluator | What it Checks | Issues |
|-----------|----------------|--------|
| HallucinationEvaluator | Non-existent package imports | H001: Package not in package.json |
| TodoEvaluator | TODO/FIXME comments | T001 (Warning): TODO, T002 (Critical): FIXME |
| IncompleteEvaluator | Incomplete implementations | I001-I002 |

The hallucination detector checks if imported packages exist in `package.json`. For example:

```typescript
// H001: Package "@fake/package" not found in package.json
import { something } from "@fake/package";
```

Some packages are allowed even if they're not in package.json: `@prisma/client`, `@prisma/sdk`, `@nestia/core`, `@nestia/fetcher`, `@nestjs/common`, `@nestjs/core`, and Node.js built-ins (`fs`, `path`, `http`, etc.)

Incomplete implementation patterns:

| Code | Severity | Pattern |
|------|----------|---------|
| I001 | Critical | `throw new Error("not implemented")` |
| I002 | Critical | `// implement this` comment |

---

## Scoring System

### Severity Weights

Each issue has a severity that affects the score:

| Severity | Weight | Description |
|----------|--------|-------------|
| Critical | -10 | Must fix - blocks production use |
| Warning | -3 | Should fix - affects quality |
| Suggestion | -1 | Nice to fix - minor improvement |

### Phase Score Calculation

```
weightedIssues = (critical × 10) + (warning × 3) + (suggestion × 1)
issueRatio = weightedIssues / totalFiles
phaseScore = max(0, min(100, 100 - issueRatio × 10))
```

### Total Score Calculation

```
totalScore = (functionality × 0.4) + (quality × 0.3) + (safety × 0.2) + (llmSpecific × 0.1)
```

If Gate fails, totalScore = 0.

### Example Calculation

Say you have 50 files, 2 critical issues, 10 warnings, and 5 suggestions:

```
weightedIssues = (2 × 10) + (10 × 3) + (5 × 1) = 20 + 30 + 5 = 55
issueRatio = 55 / 50 = 1.1
phaseScore = 100 - (1.1 × 10) = 89
```

---

## Target Folders

The tool scans specific folders based on AutoBE's output structure:

| Folder | AutoBE Phase | Contents | Evaluations |
|--------|--------------|----------|-------------|
| `src/controllers/` | Interface | API endpoint handlers | Quality, Safety, LLM |
| `src/providers/` | Realize | Business logic | Quality, Safety, LLM |
| `src/api/structures/` | Interface | DTOs, request/response types | Quality, LLM |
| `test/features/api/` | Test | E2E test files | Functionality (count only) |
| `prisma/schema/` | Database | Prisma schema files | Gate (Prisma validation) |
| `docs/analysis/` | Analyze | Requirements documents | Functionality |

Ignored paths: `**/node_modules/**`, `**/dist/**`, `**/*.d.ts`, `**/*.js` (only `.ts` files are evaluated)

---

## Output Reports

Two report files are generated:

### 1. Markdown Report (`estimate-report.md`)

Human-readable report with overall score and grade, phase-by-phase breakdown, list of all issues with locations, and summary statistics.

### 2. JSON Report (`estimate-report.json`)

Machine-readable report for CI/CD integration:

```json
{
  "targetPath": "/path/to/project",
  "totalScore": 74,
  "grade": "C",
  "phases": {
    "gate": { "passed": true, "score": 100 },
    "functionality": { "score": 90 },
    "quality": { "score": 25 },
    "safety": { "score": 100 },
    "llmSpecific": { "score": 100 }
  },
  "summary": {
    "totalIssues": 156,
    "criticalCount": 0,
    "warningCount": 147,
    "suggestionCount": 9
  }
}
```

---

## Examples

### Example 1: Evaluating Claude-generated Code

```bash
pnpm start \
  --input ~/autobe-examples/anthropic/claude-sonnet-4.5/todo \
  --output ./reports \
  --verbose
```

Output:

```
Total Score: 74/100 (Grade: C)

   Gate:          Pass
   Functionality: 90/100
   Quality:       25/100
   Safety:        100/100
   LLM Specific:  100/100

Warnings: 947
Suggestions: 9
```

### Example 2: Comparing Multiple Models

```bash
# Claude Sonnet 4.5
pnpm start -i ~/autobe-examples/anthropic/claude-sonnet-4.5/todo -o ./reports/claude

# GPT-4.1
pnpm start -i ~/autobe-examples/openai/gpt-4.1/todo -o ./reports/gpt

# Gemini 2.5 Pro
pnpm start -i ~/autobe-examples/google/gemini-2.5-pro/todo -o ./reports/gemini
```

### Example 3: CI/CD Integration

```yaml
# .github/workflows/evaluate.yml
- name: Evaluate Generated Code
  run: |
    pnpm start -i ./generated -o ./reports
    SCORE=$(cat ./reports/estimate-report.json | jq '.totalScore')
    if [ $SCORE -lt 70 ]; then
      echo "Quality score too low: $SCORE"
      exit 1
    fi
```

---

## Troubleshooting

### Gate Fails: Prisma Validation Error

If you see `[P001] Prisma schema validation failed`, check if `prisma/schema/` exists. Run `npx prisma validate` manually to see the detailed error. Make sure Prisma CLI is installed.

### Gate Fails: Type Errors

If `tsc --noEmit` returns errors, check if `tsconfig.json` exists. Run `npx tsc --noEmit` manually to see the errors. Make sure all dependencies are installed with `pnpm install`.

### No Files Found

If you see `Found 0 TypeScript files`, verify the input path is correct, check if the project follows AutoBE folder structure, and ensure files have `.ts` extension.

### Use --continue-on-gate-failure

To debug issues, run with this flag to see all evaluation results:

```bash
pnpm start -i ./project -o ./reports --continue-on-gate-failure -v
```

---

## Project Structure

The main package lives in `packages/estimate/`.

The CLI entry point is `src/bin/estimate.ts`, which handles command invocation. Argument parsing happens in `src/cli.ts`.

The core logic is split into two files: `src/core/context-builder.ts` handles project scanning and gathering file information, while `src/core/pipeline.ts` orchestrates the entire evaluation flow.

Evaluators are organized by category under `src/evaluators/`. The `base.ts` file contains base evaluator classes that others extend. Gate evaluators live in the `gate/` subdirectory, quality evaluators in `quality/`, safety evaluators in `safety/`, LLM-specific evaluators in `llm-specific/`, and functionality evaluators in `functionality/`.

Report generation is handled by `src/reporters/`. The `json.reporter.ts` generates machine-readable JSON output, and `markdown.reporter.ts` produces human-readable markdown reports.

TypeScript type definitions are in `src/types/`, and `src/index.ts` handles package exports.

Generated reports go into the `reports/` directory. Configuration for evaluation criteria is in `evaluation-criteria.json`. The standard `package.json` and `tsconfig.json` files are at the package root.

---

## License

MIT
